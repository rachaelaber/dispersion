# Do the convolution
x <- 0:(c[d_2])
x
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
x
?dnbinom
length(ex_cases)
ex_cases <- ex_cases[1:30]
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = min(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
x
dnbinom(77, means[5], dispersions[5])
for (j in x){
for (i in 1:length(means)){
print(means[i])
print(dispersions[i])
print(x[j])
print(dnbinom(x[j], mu = means[i], size = dispersions[i] ))
}
}
warnings()
x = 1:30
parms
c
parms=c(2,.1)
c=ex_cases
d_2 = 30
c
c = ex_cases[1:30]
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = min(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
ex_cases
# use first param. settings, first region and first realization as example
ex_cases <- table(factor(dfs_1[[1]][[1]]$day_of_rep,
levels=1:max(dfs_1[[1]][[1]]$day_of_rep)))
parms=c(2,.1)
c=ex_cases
d_2 = 10
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = min(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
p
means[i]
means[1]
dispersions[1]
means
ex_cases
parms=c(2,.1)
c=ex_cases
d_2 = 10
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = max(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
for (i in 2:n){
pi <- dnbinom(rx, mu = means[i], size = dispersions[1])
p = convolve(p, pi, type="open")[1:(c[d_2]+1)]
}
data.frame("x"= 0:c[d_2],"p"=p)[x == c[d_2], 2]
parms=c(2,.1)
c=ex_cases
d_2 = 30
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = max(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
for (i in 2:n){
pi <- dnbinom(rx, mu = means[i], size = dispersions[1])
p = convolve(p, pi, type="open")[1:(c[d_2]+1)]
}
data.frame("x"= 0:c[d_2],"p"=p)[x == c[d_2], 2]
parms=c(2,.1)
c=ex_cases
d_2 = 40
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = max(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
for (i in 2:n){
pi <- dnbinom(rx, mu = means[i], size = dispersions[1])
p = convolve(p, pi, type="open")[1:(c[d_2]+1)]
}
data.frame("x"= 0:c[d_2],"p"=p)[x == c[d_2], 2]
ex_cases
parms=c(2,.1)
c=ex_cases
d_2 = 22
## Xi ~ NB(p(d_2-d_i) * c_i * R, c_i * k), i = 1, ..., n, where n = 30 days in this example
## c are cases on day i, and d_i is the index for that day
## We need to compute P(X1+...+X_n = c[d_2])
R = parms[1]
k = parms[2]
n = 30            # 30 days total
d = seq(1, n)     # day index for each NB variable
means = rep(NA, times=30)
for (i in 1:30){
means[i] = max(0.1, p_tau(d_2-d[i]) * c[i] * R) # ** how else to get around some days have mean                                                         zero contribution
}
dispersions = c * k
# Do the convolution
x <- 0:(c[d_2])
rx <- rev(x)
p = dnbinom(x, mu = means[1], size = dispersions[1])
for (i in 2:n){
pi <- dnbinom(rx, mu = means[i], size = dispersions[1])
p = convolve(p, pi, type="open")[1:(c[d_2]+1)]
}
data.frame("x"= 0:c[d_2],"p"=p)[x == c[d_2], 2]
t <- conv.NB(parms=c(2,.1), c=ex_cases, d_2 = 30) # prob. of c[d_2=30] cases on day 30 if the R = 2, k = .1
source("code/prob_c[d_2].R")
# use first param. settings, first region and first realization as example
ex_cases <- table(factor(dfs_1[[1]][[1]]$day_of_rep,
levels=1:max(dfs_1[[1]][[1]]$day_of_rep)))
t <- conv.NB(parms=c(2,.1), c=ex_cases, d_2 = 30) # prob. of c[d_2=30] cases on day 30 if the R = 2, k = .1
t
optim(par = c(3,.3), fn = conv.NB,  c = ex_cases, d_2 = 30, method = "BFGS") # random starting values for R and k; what is the optimal parameter tuple given the cases on day 30
optim(par = c(10,5), fn = conv.NB,  c = ex_cases, d_2 = 30, method = "BFGS") # random starting values for R and k; what is the optimal parameter tuple given the cases on day 30
plot(seq(0.1, 3, length.out = 100), conv.NB(parms=c(seq(0.1, 3, length.out = 100),.1), c=ex_cases, d_2 = 30))
conv.NB(parms=c(seq(0.1, 3, length.out = 100),.1), c=ex_cases, d_2 = 30)
conv.NB(parms=c(.1,.1), c=ex_cases, d_2 = 30)
conv.NB(parms=c(1,.1), c=ex_cases, d_2 = 30)
conv.NB(parms=c(3,.1), c=ex_cases, d_2 = 30)
conv.NB(parms=c(3,.2), c=ex_cases, d_2 = 30)
conv.NB(parms=c(3,.2), c=ex_cases, d_2 = 30)
ex_cases[30]
conv.NB(parms=c(3,.2), c=ex_cases, d_2 = 11)
conv.NB(parms=c(3,.2), c=ex_cases, d_2 = 10)
t <- conv.NB(parms=c(2,.1), c=ex_cases, d_2 = 11) # prob. of c[d_2=30] cases on day 30 if the R = 2, k = .1
optim(par = c(10, 5), fn = conv.NB,  c = ex_cases, d_2 = 11, method = "BFGS") # random starting values for R and k; what is the optimal parameter tuple given the cases on day 30
optim(par = c(3, .5), fn = conv.NB,  c = ex_cases, d_2 = 11, method = "BFGS") # random starting values for R and k; what is the optimal parameter tuple given the cases on day 30
conv.NB(parms=c(2,.1), c=ex_cases, d_2 = 11)
conv.NB(parms=c(8,.9), c=ex_cases, d_2 = 11)
conv.NB(parms=c(11,9), c=ex_cases, d_2 = 11)
source("~/Desktop/2023_paper/code/validity_and_power_Wald.R", echo=TRUE)
setwd("~/Desktop/2023_paper")
source("~/Desktop/2023_paper/code/validity_and_power_Wald.R", echo=TRUE)
# Tabulate p-values
X <- data.frame("Type I" =  mean(pvals[which(curve_parms$theta2 == curve_parms$theta1)]),
"Power at 3" = mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 3)]),
"Power at 9" = mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 9)]))
filename <- "figures/pvals_sim_Wald_table.pdf"
pdf(filename, width = 6, height = 6)
grid.table(X)
library(gridExtra)
X <- data.frame("Type I" =  mean(pvals[which(curve_parms$theta2 == curve_parms$theta1)]),
"Power at 3" = mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 3)]),
"Power at 9" = mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 9)]))
filename <- "figures/pvals_sim_Wald_table.pdf"
pdf(filename, width = 6, height = 6)
grid.table(X)
dev.off()
round(mean(pvals[which(curve_parms$theta2 == curve_parms$theta1)]), 2)
?grid.table
X <- data.frame("Type I" =  round(mean(pvals[which(curve_parms$theta2 == curve_parms$theta1)]), 2),
"Power:3" = round(mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 3)]), 2),
"Power:9" = round(mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 9)]), 2))
filename <- "figures/pvals_sim_Wald_table.pdf"
pdf(filename, width = 6, height = 6)
grid.table(X)
dev.off()
X <- data.frame("Type I" =  round(mean(pvals[which(curve_parms$theta2 == curve_parms$theta1)]), 2),
"Power at 3" = round(mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 3)]), 2),
"Power at 9" = round(mean(pvals[which(abs(curve_parms$theta2 - curve_parms$theta1) == 9)]), 2))
filename <- "figures/pvals_sim_Wald_table.pdf"
pdf(filename, width = 6, height = 6)
grid.table(X)
dev.off()
source("~/Desktop/2023_paper/code/validity_and_power_LRT.R", echo=TRUE)
source("~/Desktop/2023_paper/code/validity_and_power_LRT.R", echo=TRUE)
source("~/Desktop/2023_paper/code/validity_and_power_Wald.R", echo=TRUE)
source("~/Desktop/2023_paper/code/validity_and_power_LRT.R", echo=TRUE)
p_vals.9 <- pvals[which(curve_parms$theta2 - curve_parms$theta1 == 9)]
pow.9 <- mean(which(pvals < 0.05))
pow.9
length(which(pvals < 0.05))
length(p_vals.9)
length(pvals)
# Tabulate Type I error rate and Power
p_vals.3 <- pvals[which(curve_parms$theta2 - curve_parms$theta1 == 3)]
length(p_vals.3)
which(p_vals.3 < 0.05)
pow.3 <- mean(p_vals.3 < 0.05)
pow.3
p_vals.9 <- pvals[which(curve_parms$theta2 - curve_parms$theta1 == 9)]
length(p_vals.9)
pow.9 <- mean(p_vals.9 < 0.05)
pow.9
X <- data.frame("Type I Error Rate" =  round(mean(pvals[which(curve_parms$theta2 == curve_parms$theta1)]), 2),
"Power at 3" = pow.3,
"Power at 9" = pow.9)
filename <- "figures/pvals_sim_LRT_table.pdf"
pdf(filename, width = 6, height = 6)
grid.table(X)
dev.off()
source("~/Desktop/2023_paper/code/validity_and_power_Wald.R", echo=TRUE)
source("~/Desktop/2023_paper/code/validity_and_power_Wald.R", echo=TRUE)
setwd("~/Desktop/2023_paper")
source("~/Desktop/2023_paper/code/lrt_counties.R", echo=TRUE)
source("~/Desktop/2023_paper/code/lrt_counties.R", echo=TRUE)
length(pvals)
dim(new_cases_subset)[1]
source("~/Desktop/2023_paper/code/LRT_scan_figure.R", echo=TRUE)
warnings()
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic")
lines(test[30:(length(test) - 30 + 1)], col = "red")
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic")
lines(test[30:(length(test) - 30 + 1)] ~ dates, col = "red")
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic")
points(test[30:(length(test) - 30 + 1)] ~ dates, col = "red")
plot(test)
test
test[30:(length(test) - 30 + 1)]
length(test[30:(length(test) - 30 + 1)])
length(lrt_stat)
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic", ylim = c(0, 500))
lines(test[30:(length(test) - 30 + 1)] ~ dates, col = "red")
par(mfrow = c(2, 1))
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic", ylim = c(0, 500))
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic", ylim = c(0, 500))
plot(test[30:(length(test) - 30 + 1)] ~ dates, col = "red")
par(mfrow = c(2, 1))
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic")
plot(test[30:(length(test) - 30 + 1)] ~ dates, col = "red")
par(mfrow = c(2, 1))
plot(lrt_stat ~ dates, type = "l", xlab = "Date", ylab = "LRT Statistic")
plot(test[30:(length(test) - 30 + 1)] ~ dates, col = "red", type = "l")
source("~/Desktop/2023_paper/code/LRT_scan_figure.R", echo=TRUE)
par(mfrow = c(2,2))
plot(new_cases_subset[1862, ] ~ days,  ylab = "Daily Incidence in FIPS 36067 (Wald test p-value = 0.017)",
xlab = "Date", ylim = c(0, 1400))
source("~/Desktop/2023_paper/code/vis_empirical_increase.R", echo=TRUE)
source("~/Desktop/2023_paper/code/vis_empirical_increase.R", echo=TRUE)
source("~/Desktop/2023_paper/code/vis_empirical_increase.R", echo=TRUE)
rm(list = ls())
graphics.off()
library(maps)
library(mapproj)
library(ggplot2)
library(viridis)
library(magrittr)
setwd("~/Desktop/2023_paper")
# This from
# https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html
data(unemp, package = "viridis")
unemp
names(unemp)
county_df <- map_data("county", projection = "albers", parameters = c(39, 45))
names(county_df) <- c("long", "lat", "group", "order", "state_name", "county")
county_df$state <- state.abb[match(county_df$state_name, tolower(state.name))]
county_df$state_name <- NULL
state_df <- map_data("state", projection = "albers", parameters = c(39, 45))
names(county_df)
choropleth <- merge(county_df, unemp, by = c("state", "county"), all.x = TRUE)
choropleth <- choropleth[order(choropleth$order), ]
p1 <- ggplot(choropleth, aes(long, lat, group = group)) +
geom_polygon(aes(fill = rate), colour = alpha("white", 1 / 2), linewidth = 0.2) +
geom_polygon(data = state_df, colour = "white", fill = NA) +
coord_fixed() +
theme_minimal() +
ggtitle("US unemployment rate by county") +
theme(
axis.line = element_blank(), axis.text = element_blank(),
axis.ticks = element_blank(), axis.title = element_blank()
) +
scale_fill_viridis(option = "magma")
View(choropleth)
# Adapt the above to plot Thanksgiving results
load("data/W_pvals_and_thetas_allcounties.Rdata")
load("data/processed_dat.RData")
dat <- cbind(populations, theta1, theta2, pvals)
dat$county <- gsub(" County", "", dat$County.Name)
dat$county <- gsub(" Parish", "", dat$county)
dat$county <- tolower(dat$county)
dat$state <- dat$State
dat$dtheta <- dat$theta1 - dat$theta2
## a clamped version of dtheta
clamp <- function(x, c) {
x[x > c] <- c
x[x < -c] <- -c
return(x)
}
dat$dtheta_clamped <- clamp(dat$dtheta, 20)
choropleth2 <- merge(county_df, dat, all.x = TRUE,
by = c("state", "county")
)
choropleth2 <- choropleth2[order(choropleth2$order), ]
p2 <- ggplot(choropleth2, aes(long, lat, group = group)) +
geom_polygon(aes(fill = dtheta_clamped), colour = alpha("white", 1 / 2), linewidth = 0.2) +
geom_polygon(data = state_df, colour = "white", fill = NA) +
coord_fixed() +
theme_minimal() +
ggtitle("Difference in k over Thanksgiving 2020") +
theme(
axis.line = element_blank(), axis.text = element_blank(),
axis.ticks = element_blank(), axis.title = element_blank()
) +
scale_fill_viridis(option = "magma")
# Save map
library(gridExtra)
filename <- "./figures/map_difftheta_US.pdf"
pdf(filename, width = 6, height = 6)
p2
dev.off()
View(choropleth)
View(choropleth2)
View(populations)
View(dat)
names(dat)
names(unemp)
dim(dat)
dim(unemp)
unemp$county_fips
View(dat)
View(unemp)
View(choropleth)
View(choropleth2)
View(unemp)
View(dat)
unemp$state_fips
tail(unemp$state_fips)
tail(unemp$county_fips
)
tail(dat$countyFIPS)
unemp$county_fips = paste(unemp$state_fips, unemp$county_fips, sep = "")
tail(unemp$county_fips)
tail(dat$countyFIPS)
# This from
# https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html
data(unemp, package = "viridis")
unemp$state_fips
statefips <- sprintf("%03d", unemp$state_fips)
head(statefips)
tail(statefips)
statefips <- sprintf("%d03", unemp$state_fips)
head(statefips)
tail(statefips)
# This from
# https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html
data(unemp, package = "viridis")
statefips <- sprintf("%d03", unemp$state_fips)
statefips
countyfips <- sprintf("%03d", unemp$county_fips)
statefips <-stri_pad_left(unemp$state_fips), 3, 0)
statefips <-stri_pad_left(unemp$state_fips, 3, 0)
library(stringi)
statefips <-stri_pad_left(unemp$state_fips, 3, 0)
statefips
head(statefips)
tail(statefips)
head(countyfips)
tail(countyfips)
# Assess associates between dtheta_clamped and demographic variables
library(stringi)
data(unemp, package = "viridis")
# Format FIPS in unemp data
countyfips <- sprintf("%03d", unemp$county_fips)
statefips <-stri_pad_left(unemp$state_fips, 3, 0)
unemp$fips <- paste(statefips, countyfips, sep = "")
View(unemp)
max(unemp$county_fips)
max(unemp$county_fips)
max(unemp$state_fips)
# This from
# https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html
data(unemp, package = "viridis")
max(unemp$state_fips)
max(unemp$county_fips)
# Assess associates between dtheta_clamped and demographic variables
library(stringi)
data(unemp, package = "viridis")
# Format FIPS in unemp data
countyfips <- sprintf("%03d", unemp$county_fips)
statefips <-stri_pad_right(unemp$state_fips, 2, 0)
unemp$fips <- paste(statefips, countyfips, sep = "")
View(unemp)
max(dat$countyFIPS)
# Format dtheta data
load("data/W_pvals_and_thetas_allcounties.Rdata")
# Format dtheta data
load("data/W_pvals_and_thetas_allcounties.Rdata")
max(fips)
fips
tail(fips)
head(fips)
fips <- stri_pad_left(fips, 5, 0)
data(unemp, package = "viridis")
statefips <-stri_pad_left(unemp$state_fips, 2, 0)
countyfips <- sprintf("%03d", unemp$county_fips)
unemp$fips <- paste(statefips, countyfips, sep = "")
View(unemp)
View(unemp)
dat <- cbind(fips, dtheta_clamped)
load("data/W_pvals_and_thetas_allcounties.Rdata")
fips <- stri_pad_left(fips, 5, 0)
dtheta <- theta1 - theta2
clamp <- function(x, c) {
x[x > c] <- c
x[x < -c] <- -c
return(x)
}
dtheta_clamped <- clamp(dtheta, 20)
dat <- cbind(fips, dtheta_clamped)
View(dat)
View(unemp)
# Assess associates between dtheta_clamped and demographic variables
library(stringi)
data(unemp, package = "viridis")
statefips <-stri_pad_left(unemp$state_fips, 2, 0)
countyfips <- sprintf("%03d", unemp$county_fips)
unemp$fips <- paste(statefips, countyfips, sep = "")
typeof(unemp$fips)
load("data/W_pvals_and_thetas_allcounties.Rdata")
fips <- stri_pad_left(fips, 5, 0)
typeof(fips)
dtheta <- theta1 - theta2
clamp <- function(x, c) {
x[x > c] <- c
x[x < -c] <- -c
return(x)
}
dtheta_clamped <- clamp(dtheta, 20)
dat <- cbind(fips, dtheta_clamped)
dim(dat)
dat <- data.frame(fips = fips, dtheta_clamped = dtheta_clamped)
names(dat)
names(unemp)
reg_df <- merge(unemp, dat, by = fips)
reg_df <- merge(unemp, dat, by = "fips")
View(reg_df)
names(reg_df)
which(unemp$fips %in% dat$fips)
which(!unemp$fips %in% dat$fips)
unemp$fips[6]
which(dat$fips == "01011")
