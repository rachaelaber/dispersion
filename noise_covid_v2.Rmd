---
title: Detecting changes in dispersion in COVID-19 incidence time series using a negative binomial model
author: "Rachael Aber"
header-includes: \usepackage{setspace}\doublespacing
output:
  html_document:
    df_print: paged
  pdf_document: default
geometry: margin=2.54cm
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, 
                      fig.align = "center", fig.width = 5, fig.height = 5)
```

# Introduction

Core challenges in epidemiology include estimating the efficacy of control measures, and understanding the impact of events of interest on incidence case counts. Allocating healthcare resources toward individuals with higher-than-average transmission risk can disproportionately reduce the population-level effective reproduction number (i.e., the average number of secondary infections caused by a randomly selected individual; Althouse et al. 2020). In other words, leveraging transmission variability among individuals to target interventions can be a highly effective strategy for reducing population spread. However, implementing and evaluating targeted control strategies has typically required detailed data on individual-level variation in transmission rates, which are often unavailable. Conversely, evaluating control strategies using only course-scale changes in incidence over time may fail to account for important parts of the social-biological context of transmission and control. The impact of non-pharmateutical interventions (NPIs; e.g., mask mandates) is often assessed in terms of changes in *mean* incidence, yet mask mandates are implemented and adhered to heterogeneously, so their effects may be context-dependent, leading to geographic and temporal variation in impact for the same policy. Similarly, the impacts of events involving congregation (such as national holidays) may result in changes in epidemic trajectories that differ based on context, and these changes may not be elucidated by simply examining aggregate mean changes or even mean changes per county. 

Metrics of population-level *variability* may be overlooked and useful ways to understand epidemic dynamics and control. By population-level variability, I mean - loosely - dispersion of data around the time-averaged trajectory. The ability to estimate changing rates of *dispersion* in epidemic time series would be a step towards a more complete view and predictive understanding of NPIs and gatherings at the individual as well as the population level. This is because individual-level heterogeneity in transmission scales up to affect population-level dynamics (Lloyd-Smith 2005), so variability in epidemic trajectories at the population level may provide information about individual-level variability in the transmission process. 

From a modern theoretical ecology perspective, investigating beyond the first moment of a process has also been identified as important: ecological experiments are typically geared towards assessing the impacts of the mean strength of causal processes, however the variance about mean effects has been mostly ignored as a driver in biological assemblages, but may be as important as the mean (Benedetti-Cecchi 2003). Similarly, inference based on variability in epidemic time series' are emerging: for instance, Graham et al. (2019) use the mean and interannual coefficient of variation of measles incidence to construct a metric indicative of where a location may be on the path to elimination of the pathogen. Sun et al. (2021) found a combination of individual-based and population-based strategies was required for SARS-CoV-2 control, further highlighting the importance of considering population-level variability and its relationship to individual-level variability. Analyzing variability in epidemic dynamics in terms of bursts of incidence is also important for planning surge capacity (Wallinga 2018).

Since evaluating control strategies using only course-scale changes in incidence over time may fail to account for parts of the context of transmission and control, we should aim to learn about individual-level heterogeneity as a means to understand control strategies more comprehensively. Similarly, we should aim to learn about individual-level heterogenity following congregation events, as estimates of individual-level variability are used to identify so-called "superspreading" dynamics: high levels of individual-level variability indicate that some individuals cause more onward transmission (via secondary infections) than others, and this in turn scales up to influence resulting incidence time series following these events. 

## Causes of overdispersed disease incidence

*Overdispersed disease incidence* is suggestive of underlying biological processes of superspreading (overdispersed *individual reproductive number*), demographic/environmental stochasticity affecting cases, or changes in propagation of the pathogen at the population level (i.e., population effective reproduction number changing in time). Note that some kinds of time dependence in the rate can cause autocorrelation, as can contagion (if it occurs outside of set periods), and heterogeneity (if an omitted variable is correlated in time) (Barron 1992).

Naively, the negative binomial distribution might accurately model a time series if there is a changing process mean: for example, if the mean of a Poisson distribution itself follows a gamma distribution, the resulting distribution is negative binomial (Cook 2009). Negative binomial regression (in contrast to Poisson regression) can account for unobserved heterogeneity, time dependence in the rate of a process and contagion that all lead to overdispersion (Barron 1992). However, issues arise when modeling the entire time series as negative binomial - namely, that autocorrelation remains unaccounted for (Barron 1992). 

Improved understanding of the causes of overdispersion in incidence may provide insight into interactions between host population structure and contagion processes, contributing to general predictive understanding in a range of systems, and understanding the limits of predictability.

Previously, efforts to quantify the processes underlying overdispersed incidence have included the study of underlying offspring distributions. Also, comparison of simulated and observed incidence time series has been used to estimate the role of environmental and demographic stochasticity in measles: across community sizes, demographic stochasticity of measles becomes more important in small human populations, where dynamics canâ€™t be described as well simply by contact rate and birth rate (Grenfell et al. 2002). The final process potentially underlying overdispersed incidence, varying population effective reproduction number, has been studied extensively in investigations into seasonal forcing of population-level effective reproduction number.

## Specific aim
  
**Using county-level SARS-CoV-2 time series from New York state, we investigate dispersion in case count time series and its relationship to a hypothesized changepoint. Specifically, we estimate and compare dispersion on either side of a putative changepoint in variability (Thanksgiving 2020), in order to test the hypothesis that dispersion significantly increases **. 

It was recently found that $k_t$, the time-varying transmission heterogeneity for COVID-19, decreased over time and was significantly associated with interventions to slow spread in Hong Kong (Adam et al. 2022). Since $k_t$ underlies incidence overdispersion, the method developed here will allow practitioners in epidemiology to explore whether local events result in a reduction in dispersion of cases, potentially modulated by changes in $k_t$. Key unanswered questions remain in developing methodology to recover incidence dispersion estimates and using these estimates to test the effect of gatherings and NPIs.  

# Data

For this project, we use county-level COVID-19 incidence data. Source: [USAFacts](https://usafacts.org/). The data is weekly case counts by US county.

Approximately one month of data on either side of November 24th, 2020 was used in the analysis. This step was done to include a set number of "waves" of incidence in the model fit to each side of the hypothesized change. However, we acknowledge that there is a trade-off between including few waves and increasing the power of the proposed test by including more time series data. 

# Methods

## Detecting variability changes in count data: the problem of population size

One might expect that in segments where the mean parameter is approximately the same, inference based on the Poisson distribution might be effective in detecting overdispersed incidence. There are several approaches to testing the Poisson assumption for count data, including testing variance-to-mean relationship, testing the probability-generating function (PGF) against an alternative, and standard goodness-of-fit tests (Karlis and Xekalaki 2000). However, an important consideration should be addressed for modeling time series of counts: the Poisson model has a spread of values that is comparatively more narrow for a process with larger mean (the mean/standard deviation ratio of a Poisson process is larger for larger total counts). For example, consider the mean to standard deviation ratios of the Poisson processes tabulated below: $\mu = \sigma^2 = 5$ and another with $\mu = \sigma^2 = 10$. For the process with a smaller mean, the mean to standard deviation ratio is smaller, whereas for the process with a larger mean, the mean to standard deviation ratio is larger. This presents an issue for using tests of Poisson variance to detect overdispersion or changes in dispersion in count time series, such as (importantly for epidemiology) incidence time series from populations of different sizes. 

```{r}
rat <- data.frame(Mean = c(5, 10), Ratio = c(2.236068, 3.162278))
kable(rat, align = "cc", caption = "Poisson mean to standard deviation ratio", digits=2)
```

To illustrate this issue, I identified segments with approximately steady mean by mean changepoint detection (performed using an information criterion, MBIC). The cpt.mean function in the changepoint package (Killick and Eckley 2014) assumes that the variance of the process is equal to one, so each New York county time series was scaled beforehand. Once these segments were identified, we used a goodness-of-fit test assuming that the Poisson mean parameter of each segment, $\lambda_i$ is unknown. This is called the variance test, and it has an optimal power property against the negative binomial alternative. The rejection region is demarcated by large values of the test statistic (Potthoff and Whittinghill 1966), and values of the test statistic increase with population size.
  
```{r, fig.cap="Average Pothoff-Whittinghill (PW) statistic for each NY county plotted against county population size."}

library(tidyverse)
library(changepoint)

filename <- "data/processed_dat.Rdata"
load(filename)
rm(filename)

scaled_cases <- map(as.data.frame(t(new_cases_subset)), ~ as.vector(scale(.)))

ch_pts <- map(scaled_cases, ~ cpt.mean(., class = TRUE, penalty = "MBIC", pen.value = 0, method = "PELT", param.estimates = TRUE))


segments <- map2(.x = ny_df_t$DailyNewConfirmedCases[-c(2, 8, 21, 49, 54, 62)], .y = ny_df_t$ch_pts[-c(2, 8, 21, 49, 54, 62)], ~ get_segments(series = .x, changepoints = .y))

mean_pw <- map(segments, ~ get_mean_pw(.))

# Plot the mean PW statistic v. population size

plot(log(ny_df_t$populations[-c(2, 8, 21, 49, 54, 62)]), log(unlist(mean_pw)), xlab="Log Population Size", ylab=" Log Mean PW Statistic", col=2)
```

Methods designed specifically for count data show a similar dependence on population size. The cpt.meanvar function (changepoint package) is designed to detect changes in mean and variance in count data. The number of changepoints identified for a time series increases with its population size.

```{r, fig.cap="Log of the number of changepoints identified by cpt.meanvar in each NY county plotted against log of county population size."}

ny_df_t$ch_pts2 <- map(as.data.frame(new_cases_subset), ~ cpts(cpt.meanvar(., class = TRUE, penalty = "MBIC", pen.value = 30, method = "PELT", test.stat = "Poisson", param.estimates = TRUE, minseglen = 2))) 
ny_df_t$n_ch_pts2 <- map(ny_df_t$ch_pts2, ~length(unlist(.)))

plot(log(ny_df_t$populations), log(unlist(ny_df_t$n_ch_pts2)), xlab="Log Population Size", ylab=" Log Number of Changepoints", col=2)
```

## Modeling proccess mean

We propose a generalized linear model (GLM) approach to quantify incidence dispersion in segments of a time series, using a single parameter, $\theta$, while separately accounting for population size in the model. Another approach is to fit one model to the entire time series accounting for autocorrelation using a quasi-likelihood approach using a specified matrix V (Barron 1992). We account for unobserved heterogeneity, contagion, and time dependence in rate through specifying a changing process mean (to account for autocorrelation) and specifying a overdispersed conditional distribution (to account for overdispersion).

A recently proposed negative binomial regression model for time series of counts also accommodates serial dependence (Davis and Wu 2009). The approach presented here is similar, but we use a linear predictor, $\eta$, that includes a generalized additive model (GAM) with one explanatory variable, time. In the GAM framework, the function for a predictor can be represented using spline functions: 

 $$g(E[Y_i]) = f(t_i) = \sum_m^M\beta_m h_m(t_i)$$
 $$log(E[Y_i]/n_i) = \sum_m^M\beta_m h_m(t_i)$$

Natural splines are cubic splines which are linear outside of the boundary knots (Perperoglou et al. 2019). The degrees of freedom were chosen due to **

$$g(E[Y_i]) = \beta_1h_1(t_i) + \beta_2h_2(t_i) + \beta_3h_3(t_i)$$
$$log(E[Y_i]/n_i) = \beta_1h_1(t_i) + \beta_2h_2(t_i) + \beta_3h_3(t_i)$$
```{r, fig.cap="County case count time series for each New York county with the hypothesized Thanksgiving changepoint represented by the black vertical line (L), and total New York COVID-19 case count time series (R)."}

inds <- seq(start_date, end_date, by = "day")

plot(new_cases_subset[state_index[1],], type = "l", xaxt = "n", ylab = "Daily Incidence", xlab = "Date")
for (i in 2:length(state_index)){
  lines(new_cases_subset[state_index[i],], col=sample(rainbow(10)))
}
abline(v=inds[30], lwd=2)
axis(1, inds, inds, cex.axis = .7)

ny_col_sum <- matrix(unlist(ny_df_t$DailyNewConfirmedCases), nrow=62,
                     ncol=length(ny_df_t$DailyNewConfirmedCases[[1]]))
ny_cases <- colSums(ny_col_sum)
plot(ny_cases ~ inds, type="l", col=6, xlab="Date", ylab="Daily Incidence")
abline(v=inds[30], lwd=2)
```

```{r, fig.cap="County case count time series for each Florida county with the hypothesized Thanksgiving changepoint represented by the black vertical line (L), and total Florida COVID-19 case count time series (R)."}

par(mfrow=c(1,2))
# By visual inspection 
plot(fl_df_t$DailyNewConfirmedCases[[1]] ~ inds, type="l", ylim=c(0, 5000), xaxt="n", ylab="Daily Incidence", xlab="Date")
for (i in 2:dim(fl_df_t)[1]){
  lines(fl_df_t$DailyNewConfirmedCases[[i]] ~ inds, col=sample(rainbow(10)))
}
abline(v=inds[30], lwd=2)
axis(1, inds, inds, cex.axis = .7)

fl_col_sum <- matrix(unlist(fl_df_t$DailyNewConfirmedCases), nrow=62,
                     ncol=length(fl_df_t$DailyNewConfirmedCases[[1]]))
fl_cases <- colSums(fl_col_sum)
plot(fl_cases ~ inds, type="l", col=6, xlab="Date", ylab="Daily Incidence")
abline(v=inds[30], lwd=2)
```
## IRLS 

Briefly, what iteratively reweighted least squares (IRLS) does is compute an adjusted dependent variable, compute a weight matrix, and compute (using weighted least squares) coefficients and therefore the linear predictor as well. 

In irls.nb.1 in the NBPSeq R package (*cite):

1. Initial mu (either supplied or computed) and eta = log(mu/s).
2. z = eta - offset + (y - mu)/mu
3. w = drop(\mu/sqrt(varmu))
4. call Fortran code to perform weighted LS. Weighted least squares produces: $\hat{\beta} = (X^TWX)^{-1}X^TWY$, where weight is based on the variance of the observation. 
5. Then new values of mu and eta from new coefficients.

Within diy_spl_fit, fit.nb.regression is run and the standard error of param theta is computed by inverting info matrix, take [1,1] entry and take square root. Within fit.nb.regression, kappa is optimized, then irls.nb.1 (the IRLS portion) is called, then additional output - like the information matrix - is added. 

## Inclusion of an offset term in the model  

Regardless of whether the data is modeled as negative binomial or Poisson around the process mean (and regardless of whether quasi-likelihood methods are used), it's crucial to incorporate an offset term - sometimes called an exposure variable - in order to directly model counts (here, COVID-19 cases) per unit of observation (here, per individual). Essentially, instead of focusing on the infection rate, our model allows the focus to be on infection rate per person. In other words, an offset term was added to the model to account for case counts resulting from different population sizes - an offset for population size means that incidence dispersion properties may be assessed *while accounting for a population effect*.

Note that if the offset model were perfect, the estimated coefficient on log population would be exactly one:
$$log(E[Y_i]/n_i) = \sum_m^M\beta_m h_m(t_i)$$
, where $n_i$ is the population size, $Y_i$ is incidence, and $t_i$ is the time point.

In this case, a log link function is used:
  
  $$log(E[Y_i])-log(n_i) = \sum_m^M\beta_m h_m(t_i)$$
  $$log(E[Y_i]) =  \sum_m^M\beta_m h_m(t_i) + log(n_i)$$

## Identifying an appropriate model of the conditional distribution 

```{r}
disp_stats <- c()
for (i in 1:dim(ny_fl_df_t)[1]){
  Y <- ny_fl_df_t$DailyNewConfirmedCases[[i]]
  spline_mod.1.pois <- spl_fit_pois(Y = Y, population=ny_fl_df_t$populations[i], inds=1:(length(Y)/2))
  disp_stats <- c(disp_stats, spline_mod.1.pois$deviance/spline_mod.1.pois$df.residual)
  #print(summary(spline_mod.1.nb)$coefficients[,4])
}
```

```{r}
source("code/squared.res_v_squared.fitted.R")
```

```{r, fig.cap="Average squared residual per bin v. fitted bin mean for Poisson model (L) and negative binomial model (R) with number of bins = 25 (observations per bin = 11). Visualizations displayed for a randomly chosen counties"}
par(mfrow=c(1,2))
i <- 2
Y <- ny_fl_df_t$DailyNewConfirmedCases[[i]]
spline_mod.1.pois <- spl_fit_pois(Y = Y, population=ny_fl_df_t$populations[i], inds=1:(length(Y)/2))
spline_mod.1.nb <- diy_spl_fit(Y = Y, population=ny_fl_df_t$populations[i], inds=1:(length(Y)/2), df = 3)
bin_it(fits=spline_mod.1.pois$fitted.values, y=Y[1:(length(Y)/2)], nbins=25)
abline(0,1)
abline(0,2, col=3)
bin_it(fits=spline_mod.1.nb$mu, y=Y[1:(length(Y)/2)], nbins=25)
abline(0,1)
abline(0, 2, col=3)

```

To identify an appropriate conditional distribution, the value of Poisson model residual deviance (D) divided by its degrees of freedom (df) was used to verify that overdispersion at the time point level is present under this model and found that only `r sum(disp_stats <=1)` dispersion statistics were less than or equal to one. Since residual deviance of a model is expected to be asymptotically distributed as $\chi^2_{df}$, where the degrees of freedom are the number of observations minus the number of estimated parameters, an indication that a model may be underestimating dispersion (that the data is inconsistent with the model) is that D/df >> 1. 

Directly comparing between the quasi-Poisson and the Poisson models via deviance residuals is not possible because the deviance residuals are identical. 

```{r, fig.cap="Poisson and negative binomial fits to incidence data prior to Thanksgiving 2020 in a single county time series."}

source("code/check_nb_estimation.R")
```

Therefore, to evaluate whether the negative binomial conditional distribution is needed (opposed to a Poisson/quasi-Poisson conditional distribution with the same model of process mean), I inspected the mean-variance relationships using a diagnostic plot as in Ver Hoef and Boveng (2007). 

In the quasi-Poisson model (called the linear negative binomial specification by Barron (1992)), the variance is a linear function of the mean, with a slope of one representing the Poisson special case. By contrast, the negative binomial model may more appropriately represent the data when the variance has approximately a quadratic relationship with the mean. Since the true mean and variance of the data set is unknown, we approximate these quantities with estimates $\hat{\mu}$ by and $\widehat{\sigma^2}$, respectively. These quantities are estimated using both the Poisson and the negative binomial model. Variance can be estimated by averaging squared residuals in each fitted mean category. The number of fitted mean categories (bins) was chosen such that there are an adequate number of observations informing each variance estimate, and also an adequate number of variance estimates to visualize whether a trend is present. Visualization of individual squared residuals is also possible, but these may not be reliable estimates of the variance.

## Proposed hypothesis test

```{r, fig.cap="Example of empirical increase in dispersion at the hypothesized Thanksgiving 2020 changepoint."}
plot(ny_df_t$DailyNewConfirmedCases[[14]] ~ inds, xaxt="n", ylab="Daily Incidence", xlab="Date")
abline(v=inds[30], col=2)
axis(1, inds, inds, cex.axis = .7)
```

We tested the hypothesis that the incidence dispersion parameter decreases at the Thanksgiving changepoint of interest: $\theta_1 > \theta_2$. Hypothesis tests were conducted to assess whether $\theta$ is smaller (more variable incidence) after Thanksgiving 2020. The Wald test is used, where the estimates of $\theta$ in different segments are considered independent. This approach is possible because we use maximum likelihood estimates of $\theta$ (found by alternating between fitting a GLM for a given theta and then (for fixed means) estimating $\theta$). The standard error of $\theta$ is found utilizing the observed information (Venables and Ripley 2002).

$$Cov(\hat{\theta_1},\hat{\theta_2}) = 0 $$
The form of the Wald test statistic is:

$$\frac{\hat{\theta_1} -\hat{\theta_2}}{SE(\hat{\theta_1} -\hat{\theta_2})}$$
$$ = \frac{\hat{\theta_1} -\hat{\theta_2}}{\sqrt{\widehat{Var(\hat{\theta_1})} + \widehat{Var(\hat{\theta_2})}}}$$
# Results 

## Thanksgiving 2020: New York and Florida

First, for every county, the model was fit separately to either side of November 25th, 2020 - where the 25th was included in the second segment -  in order to compute values of $\hat{\theta_1}$ and $\hat{\theta_2}$. A test of the one-sided upper alternative that the difference between before and after is positive (lower theta after Thanksgiving) was performed. 

```{r, fig.cap = "P-values resulting from testing whether the dispersion parameter is smaller after Thanksgiving in each county in New York and Florida."}

# Wald test for NY Thanksgiving in 2020 - doesn't converge for 13th and 50th NY counties
diff_thetas <- c()
t_ps <- c()
for (i in 1:dim(ny_df_t)[1]){ 
  if (i == 13 | i ==50){
    p = NA
    theta = NA
  }
  else{
    p <- W(y=ny_df_t$DailyNewConfirmedCases[[i]], pop=ny_df_t$populations[i], split=30)
    theta1 <- diy_spl_fit(Y=ny_df_t$DailyNewConfirmedCases[[i]], population =
                            ny_df_t$populations[i],
                          inds=1:(length(ny_df_t$DailyNewConfirmedCases[[i]])/2), df = 3)$theta
    theta2 <- diy_spl_fit(Y=ny_df_t$DailyNewConfirmedCases[[i]], population =
                            ny_df_t$populations[i],
                          inds=(length(ny_df_t$DailyNewConfirmedCases[[i]])/2
                                +1):length(ny_df_t$DailyNewConfirmedCases[[i]]), df = 3)$theta
  }
  t_ps <- c(t_ps, p)
  diff_thetas <- c(diff_thetas, abs(theta1 - theta2))
}
ny_df_t$p <- t_ps
ny_df_t$diff_thetas <- diff_thetas


# Wald test for FL Thanksgiving in 2020
diff_thetas <- c()
t_ps <- c()
for (i in 1:dim(fl_df_t)[1]){ 
  p <- W(y=fl_df_t$DailyNewConfirmedCases[[i]], pop=fl_df_t$populations[i], split=30)
  theta1 <- diy_spl_fit(Y=fl_df_t$DailyNewConfirmedCases[[i]], population = fl_df_t$populations[i],
                       inds=1:(length(fl_df_t$DailyNewConfirmedCases[[i]])/2), df = 3)$theta
  theta2 <- diy_spl_fit(Y=fl_df_t$DailyNewConfirmedCases[[i]], population = fl_df_t$populations[i],
                       inds=(length(fl_df_t$DailyNewConfirmedCases[[i]])/2 +
                               1):length(fl_df_t$DailyNewConfirmedCases[[i]]), df = 3)$theta
  t_ps <- c(t_ps, p)
  diff_thetas <- c(diff_thetas, abs(theta1-theta2))
}
fl_df_t$p <- t_ps
fl_df_t$diff_thetas <- diff_thetas

#p_adj_t <- p.adjust(t_ps, method = "BH")
par(mfrow=c(2,2))
# Histogram of p-values NY
hist(ny_df_t$p, col=2, breaks=20, xlab="p-values", main=NULL)
# Power lower based on population? NY
plot(ny_df_t$p ~ ny_df_t$populations)
# Histogram of p-values FL
hist(fl_df_t$p, col=2, breaks=20, xlab="p-values", main=NULL)
# Power lower based on population? FL
plot(fl_df_t$p ~ fl_df_t$populations)

```

```{r, fig.width=7, fig.height=7}
# Plot fitted curves for the counties with significant p-values

ny_indeces <- which(ny_df_t$p < 0.05)

par(mar=c(1,1,1,1))
par(mfrow=c(6,2))

for (i in ny_indeces){

mu <- diy_spl_fit(Y = ny_df_t$DailyNewConfirmedCases[[i]], population = ny_df_t$populations[i], inds=1:length(ny_df_t$DailyNewConfirmedCases[[i]]), df = 3)$mu

plot(1:length(ny_df_t$DailyNewConfirmedCases[[i]]), ny_df_t$DailyNewConfirmedCases[[i]], ylim=c(0,500))
lines(mu, col=2)
}

fl_indeces <- which(fl_df_t$p < 0.05)

par(mar=c(1,1,1,1))
par(mfrow=c(6,2))

for (i in fl_indeces){

mu <- diy_spl_fit(Y = fl_df_t$DailyNewConfirmedCases[[i]], population = fl_df_t$populations[i], inds=1:length(fl_df_t$DailyNewConfirmedCases[[i]]), df = 3)$mu

plot(1:length(fl_df_t$DailyNewConfirmedCases[[i]]), fl_df_t$DailyNewConfirmedCases[[i]])
lines(mu, col=2)
}

```
https://cran.r-project.org/web/packages/tigris/readme/README.html

```{r, echo=F, fig.cap="P-values for each New York county resulting from the test of the one-sided upper alternative that time series dispersion is greater after Thanksgiving 2020"}

ny <- counties("New York")

p <- round(ny_df_t$p, digits=1)
p[is.na(p)] <- "na"
ny$p <- p

ggplot(ny) +
  geom_sf(aes(fill = p)) 

fl <- counties("Florida")

p <- round(fl_df_t$p, digits=1)
p[is.na(p)] <- "na"
fl$p <- p 

ggplot(fl) +
  geom_sf(aes(fill=p))

```
# Simulation

## Validity of the hypothesis test

To ensure that the hypothesis testing framework is valid, a simulation with known (equal) values of theta on either side of Thanksgiving 2020 was run in order to ensure that the distribution of resulting p-values from the test of the upper alternative is approximately uniform on the interval from zero to one. In both the validity and power simulations, representative epidemic curves are used. The area under these curves (final outbreak size) is set to be the population size divided by ten. 

To implement these constraints on the integral:
we use a normalizing constant of (n/10) * sqrt(2*pi* $\sigma^2$)

$$ \int_0^\infty {(n/10)/\sqrt{2\pi(\sigma^2))} e^{-(t-61)^2/(2\sigma^2)}}dt = n/10$$
So, we have the desired Gaussian function:
$$ \mu_t = (n/10)/\sqrt{2\pi\sigma^2} e^{-(t-61)^2/(2\sigma^2)} $$
So that integration will result in a final outbreak size of n/10.

```{r, fig.cap = "Representative epidemic curve to inform the validity simulation and the power simulation."}

curve_parms <- data.frame(n = rep(mean(ny_df_t$populations), times = 300),  b = rep(30, times = 300), c = rep(200, times = 300), theta1 = c(seq(4, 10, length=100), seq(4, 10, length=100), seq(9, 3, length=100)), theta2 = c(seq(4, 10, length=100), seq(12, 6, length=100), seq(2, 8, length=100)))

plot(1:60, u_t(n = curve_parms$n[1], t = 1:60, b = curve_parms$b[1], c=curve_parms$c[1]), ylim = c(0, 1000))
```

```{r, fig.cap="Validity simulation p-values."}
# Wald test for simulated data - same process as was done to real time series from NY counties

splits <- c(20, 30, 40)

p_vals <- c()
diff_vals <- c()

for (k in splits){
  for (i in 1:100){
    
  sides <- rerun(sim_sides(parms=curve_parms[i,], time=1:60), .n=10)
  ps <- map(sides, ~ W(y=., pop=curve_parms$n[i], split=k, verbose=FALSE))
  
  p_vals <- c(p_vals, ps)
  diff_vals <-c(diff_vals, rep(abs(curve_parms[i,4]-curve_parms[i,5]), times=10))
  
  }
}

hist(unlist(p_vals), col=2, breaks=20, xlab="p-values", main=NULL)

```

## Power of the hypothesis test

To ensure that the hypothesis test has adequate power, a simulation using unequal values of theta on either side of Thanksgiving was repeatedly run in order to ensure that the distribution of resulting p-values is heavily right-skewed. 

```{r, fig.cap="Power simulation p-values."}
p_pows <- c()
diff_pows <- c()

for (k in splits){
  for (i in 100:300){ # change to 100:300 for two-sided hyp test
    
    sides <- rerun(sim_sides(parms = curve_parms[i,], time=1:60), .n=10)
    ps <- map(sides, ~ W(y=., pop=curve_parms$n[i], split=k, verbose=FALSE))
    
    p_pows <- c(p_pows, ps)
    diff_pows <- c(diff_pows, rep((abs(curve_parms[i,4]-curve_parms[i,5])), times=10))
    
  }
}

# Histogram of p-values
hist(unlist(p_pows), col=3, breaks=20, xlab="p-values", main=NULL)
```

```{r}
plot(c(diff_vals, diff_pows), c(p_vals, p_pows))
```

The resulting probabilities of rejection are visualized below using one of the Gaussian epidemic curves to represent the process mean. As hoped, the proportion of tests that reject is around 0.05 for those instances where the null is true (probability of the normal variable having 0.05 of the probability density below is 0.05). Additionally, the proportion of tests that reject is high in instances where the alternative hypothesis is true. 


## References 

Adam, D., Gostic, K., Tsang, T., Wu, P., Lim, W. W., Yeung, A., Wong, J., Lau, E., Du, Z., Chen, D., Ho, L.-M., MartÃ­n-SÃ¡nchez, M., Cauchemez, S., Cobey, S., Leung, G., & Cowling, B. (2022). Time-varying transmission heterogeneity of SARS and COVID-19 in Hong Kong. https://doi.org/10.21203/rs.3.rs-1407962/v1

Cook, J. D. (n.d.). Notes on the Negative Binomial Distribution. 5.

Davis, R. A., & Wu, R. (2009). A negative binomial model for time series of counts. Biometrika, 96(3), 735â€“749.

Graham, M., Winter, A. K., Ferrari, M., Grenfell, B., Moss, W. J., Azman, A. S., Metcalf, C. J. E., & Lessler, J. (2019). Measles and the canonical path to elimination. Science, 364(6440), 584â€“587. https://doi.org/10.1126/science.aau6299

Karlis, D., & Xekalaki, E. (2000). A Simulation Comparison of Several Procedures for Testing the Poisson Assumption. Journal of the Royal Statistical Society: Series D (The Statistician), 49(3), 355â€“382. https://doi.org/10.1111/1467-9884.00240

Killick, R., & Eckley, I. A. (2014). changepoint: An R Package for Changepoint Analysis. Journal of Statistical Software, 58, 1â€“19. https://doi.org/10.18637/jss.v058.i03

Lloyd-Smith, J. O., Schreiber, S. J., Kopp, P. E., & Getz, W. M. (2005). Superspreading and the effect of individual variation on disease emergence. Nature, 438(7066), Article 7066. https://doi.org/10.1038/nature04153

Perperoglou, A., Sauerbrei, W., Abrahamowicz, M., & Schmid, M. (2019). A review of spline function procedures in R. BMC Medical Research Methodology, 19(1), 46. https://doi.org/10.1186/s12874-019-0666-3

Potthoff, R. F., & Whittinghill, M. (1966). Testing for homogeneity. II. The Poisson distribution. Biometrika, 53(1), 183â€“190.
Transmission heterogeneities, kinetics, and controllability of SARS-CoV-2. (n.d.). https://doi.org/10.1126/science.abe2424

Transmission heterogeneities, kinetics, and controllability of SARS-CoV-2. (n.d.). https://doi.org/10.1126/science.abe2424

Venables W.N., Ripley B.D. (2002). Modern Applied Statistics with S, Fourth edition. Springer, New York. ISBN 0-387-95457-0, https://www.stats.ox.ac.uk/pub/MASS4/.

Ver Hoef, J. M., & Boveng, P. L. (2007). QUASI-POISSON VS. NEGATIVE BINOMIAL REGRESSION: HOW SHOULD WE MODEL OVERDISPERSED COUNT DATA? Ecology, 88(11), 2766â€“2772. https://doi.org/10.1890/07-0043.1

Wallinga, J. (2018). Metropolitan versus small-town influenza. Science. https://doi.org/10.1126/science.aav1003